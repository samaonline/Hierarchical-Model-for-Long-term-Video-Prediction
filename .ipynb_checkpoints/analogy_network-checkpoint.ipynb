{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T08:29:50.360225",
     "start_time": "2017-05-07T01:29:50.208659-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import random\n",
    "from scipy.misc import imread\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-06T07:32:46.341952",
     "start_time": "2017-05-06T00:32:45.966162-07:00"
    },
    "code_folding": [
     45
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, filter_size, num_filters, stride, name, padding='SAME', groups=1, trainable=True):\n",
    "    input_channels = int(x.get_shape()[-1])\n",
    "\n",
    "    # Create lambda function for the convolution\n",
    "    convolve = lambda x, W: tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        # Create tf variables for the weights and biases of the conv layer\n",
    "        weights = tf.get_variable('W',\n",
    "                                  shape=[filter_size, filter_size, input_channels // groups, num_filters],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  trainable=trainable)\n",
    "        biases = tf.get_variable('b', shape=[num_filters], trainable=trainable, initializer=tf.zeros_initializer())\n",
    "\n",
    "        if groups == 1:\n",
    "            conv = convolve(x, weights)\n",
    "\n",
    "        else:\n",
    "            # Split input and weights and convolve them separately\n",
    "            input_groups = tf.split(x, groups, axis=3)\n",
    "            weight_groups = tf.split(weights, groups, axis=3)\n",
    "            output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\n",
    "\n",
    "            # Concat the convolved output together again\n",
    "            conv = tf.concat(output_groups, axis=3)\n",
    "\n",
    "        return tf.nn.relu(conv + biases)\n",
    "\n",
    "def deconv_layer(x, filter_size, num_filters, stride, name, padding='SAME', relu=True):\n",
    "    activation = None\n",
    "    if relu:\n",
    "        activation = tf.nn.relu\n",
    "    return tf.layers.conv2d_transpose(x, num_filters, filter_size, stride, padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=activation, name=name)\n",
    "    \n",
    "def fc(x, num_out, name, relu=True, trainable=True):\n",
    "    num_in = int(x.get_shape()[-1])\n",
    "    with tf.variable_scope(name):\n",
    "        weights = tf.get_variable('W', shape=[num_in, num_out], initializer=tf.contrib.layers.xavier_initializer(), trainable=trainable)\n",
    "        biases = tf.get_variable('b', [num_out], initializer=tf.zeros_initializer(), trainable=trainable)\n",
    "        x = tf.matmul(x, weights) + biases\n",
    "        if relu:\n",
    "            x = tf.nn.relu(x) \n",
    "    return x\n",
    "\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
    "    return tf.nn.local_response_normalization(x, depth_radius=radius, alpha=alpha, beta=beta, bias=bias, name=name)\n",
    "\n",
    "def max_pool(x, filter_size, stride, name=None, padding='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_size, filter_size, 1], strides=[1, stride, stride, 1], padding=padding, name=name)\n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "    return tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "def vgg(input, process_input=True):\n",
    "    if process_input:\n",
    "        VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "        \n",
    "        # Convert RGB to BGR and subtract mean\n",
    "        red, green, blue = tf.split(input, 3, axis=3)\n",
    "        input = tf.concat([\n",
    "            blue - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "            red - VGG_MEAN[2],\n",
    "        ], axis=3)\n",
    "        \n",
    "    pool_ = lambda x: max_pool(x, 2, 2)\n",
    "    conv_ = lambda x, output_depth, name: conv(x, 3, output_depth, 1, name=name)\n",
    "    \n",
    "    conv_1_1 = conv_(input, 64, 'conv1_1')\n",
    "    conv_1_2 = conv_(conv_1_1, 64, 'conv1_2')\n",
    "\n",
    "    pool_1 = pool_(conv_1_2)\n",
    "\n",
    "    conv_2_1 = conv_(pool_1, 128, 'conv2_1')\n",
    "    conv_2_2 = conv_(conv_2_1, 128, 'conv2_2')\n",
    "\n",
    "    pool_2 = pool_(conv_2_2)\n",
    "\n",
    "    conv_3_1 = conv_(pool_2, 256, 'conv3_1')\n",
    "    conv_3_2 = conv_(conv_3_1, 256, 'conv3_2')\n",
    "    conv_3_3 = conv_(conv_3_2, 256, 'conv3_3')\n",
    "\n",
    "    pool_3 = pool_(conv_3_3)\n",
    "\n",
    "    conv_4_1 = conv_(pool_3, 512, 'conv4_1')\n",
    "    conv_4_2 = conv_(conv_4_1, 512, 'conv4_2')\n",
    "    conv_4_3 = conv_(conv_4_2, 512, 'conv4_3')\n",
    "\n",
    "    pool_4 = pool_(conv_4_3)\n",
    "\n",
    "    conv_5_1 = conv_(pool_4, 512, 'conv5_1')\n",
    "    conv_5_2 = conv_(conv_5_1, 512, 'conv5_2')\n",
    "    conv_5_3 = conv_(conv_5_2, 512, 'conv5_3')\n",
    "\n",
    "    pool_5 = pool_(conv_5_3)\n",
    "    flattened = tf.contrib.layers.flatten(pool_5)\n",
    "\n",
    "    fc_6 = dropout(fc(flattened, 4096, 'fc6'), 0.5)\n",
    "    fc_7 = fc(fc_6, 4096, 'fc7', relu=False)\n",
    "    return fc_7\n",
    "\n",
    "def vgg_simple(input):\n",
    "    pool_ = lambda x: max_pool(x, 2, 2)\n",
    "    conv_ = lambda x, output_depth, name: conv(x, 3, output_depth, 1, name=name)\n",
    "    \n",
    "    conv_1_1 = conv_(input, 16, 'conv1_1')\n",
    "    pool_1 = pool_(conv_1_1)\n",
    "\n",
    "    conv_2_1 = conv_(pool_1, 32, 'conv2_1')\n",
    "    pool_2 = pool_(conv_2_1)\n",
    "\n",
    "    conv_3_1 = conv_(pool_2, 64, 'conv3_1')\n",
    "    pool_3 = pool_(conv_3_1)\n",
    "\n",
    "    conv_4_1 = conv_(pool_3, 64, 'conv4_1')\n",
    "    pool_4 = pool_(conv_4_1)\n",
    "\n",
    "    conv_5_1 = conv_(pool_4, 64, 'conv5_1')\n",
    "    pool_5 = pool_(conv_5_1)\n",
    "    \n",
    "    flattened = tf.contrib.layers.flatten(pool_5)\n",
    "    fc_6 = dropout(fc(flattened, 4096, 'fc6'), 0.5)\n",
    "    fc_7 = fc(fc_6, 4096, 'fc7', relu=False)\n",
    "    return fc_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-06T07:36:57.533285",
     "start_time": "2017-05-06T00:36:56.650191-07:00"
    },
    "code_folding": [
     51
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self):\n",
    "        self.train_variables = []\n",
    "        self.has_defined_layers = False\n",
    "        self.has_defined_C1 = False\n",
    "    \n",
    "    def init_network(self, discriminator):\n",
    "        self.p_t_n = tf.placeholder(tf.float32, [None, 224, 224,L])\n",
    "        self.p_t = tf.placeholder(tf.float32, [None, 224, 224,L])\n",
    "        self.x_t = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        self.x_t_n = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        x_t_n_predicted = self.get_output_tensor(self.p_t_n, self.p_t, self.x_t)\n",
    "        mean_l2 = lambda x, y: tf.reduce_mean(tf.squared_difference(x, y))\n",
    "        l2_loss = mean_l2(self.x_t_n, x_t_n_predicted)\n",
    "        feat_loss = mean_l2(self.C1(self.x_t), self.C1(x_t_n_predicted))\n",
    "        adv_loss = -tf.reduce_mean(tf.log(discriminator.get_output_tensor(x_t_n_predicted, self.p_t_n)))\n",
    "        self.loss = 100*l2_loss + 100*feat_loss + 0.05 * adv_loss\n",
    "        self.opt = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(self.loss, var_list=self.train_variables)\n",
    "        with tf.name_scope('generator'):\n",
    "            l2_loss_summ = tf.summary.scalar('l2_loss', l2_loss)\n",
    "            feature_loss_summ = tf.summary.scalar('feature_loss', feat_loss)\n",
    "            adversarial_loss_summ = tf.summary.scalar('adversarial_loss', adv_loss)\n",
    "            loss_summ = tf.summary.scalar('loss', self.loss)\n",
    "            self.summaries = tf.summary.merge([l2_loss_summ, feature_loss_summ, adversarial_loss_summ, loss_summ])\n",
    "        \n",
    "    def get_output_tensor(self, p_t_n, p_t, x_t):\n",
    "        with tf.variable_scope('generator', reuse=self.has_defined_layers):\n",
    "            p_t_n_latent = self.f_pose(p_t_n)\n",
    "            latent = p_t_n_latent - self.f_pose(p_t, force_reuse=True) + self.f_img(x_t)\n",
    "            output = self.f_dec(latent)\n",
    "        if not self.has_defined_layers:\n",
    "            self.train_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "        self.has_defined_layers = True\n",
    "        return output\n",
    "        \n",
    "    def f_pose(self, input, force_reuse=False):\n",
    "        '''\n",
    "        Applies f_pose function to the input tensor to get an output. Should be similar to VGG architecture\n",
    "        '''\n",
    "        with tf.variable_scope('f_pose', reuse=(self.has_defined_layers or force_reuse)):\n",
    "            return vgg_simple(input)\n",
    "        \n",
    "    def f_img(self, input):\n",
    "        '''\n",
    "        Applies f_img function to the input tensor to get an output. Should be exactly VGG architecture\n",
    "        '''\n",
    "        with tf.variable_scope('f_img', reuse=self.has_defined_layers):\n",
    "            return vgg(input, process_input=True)\n",
    "        \n",
    "    def f_dec(self, input):\n",
    "        '''\n",
    "        Applies f_dec function to the input tensor to get an output.\n",
    "        '''\n",
    "        with tf.variable_scope('f_dec', reuse=self.has_defined_layers):\n",
    "            reshaped = tf.reshape(input, shape=[tf.shape(input)[0], 1, 1, 4096])\n",
    "            deconv_6_2 = deconv_layer(reshaped, 7, 128, 1, 'deconv6_2', padding='VALID')\n",
    "            deconv_6_1 = deconv_layer(deconv_6_2, 3, 128, 2, 'deconv6_1')\n",
    "\n",
    "            deconv_5_2 = deconv_layer(deconv_6_1, 3, 128, 1, 'deconv5_2')\n",
    "            deconv_5_1 = deconv_layer(deconv_5_2, 3, 128, 2, 'deconv5_1')\n",
    "\n",
    "            deconv_4_3 = deconv_layer(deconv_5_1, 3, 128, 1, 'deconv4_3')\n",
    "            deconv_4_2 = deconv_layer(deconv_4_3, 3, 128, 1, 'deconv4_2')\n",
    "            deconv_4_1 = deconv_layer(deconv_4_2, 3, 64, 2, 'deconv4_1')\n",
    "\n",
    "            deconv_3_3 = deconv_layer(deconv_4_1, 3, 64, 1, 'deconv3_3')\n",
    "            deconv_3_2 = deconv_layer(deconv_3_3, 3, 64, 1, 'deconv3_2')\n",
    "            deconv_3_1 = deconv_layer(deconv_3_2, 3, 32, 2, 'deconv3_1')\n",
    "\n",
    "            deconv_2_2 = deconv_layer(deconv_3_1, 3, 32, 1, 'deconv2_2')\n",
    "            deconv_2_1 = deconv_layer(deconv_2_2, 3, 16, 2, 'deconv2_1')\n",
    "\n",
    "            deconv_1_2 = deconv_layer(deconv_2_1, 3, 16, 1, 'deconv1_2')\n",
    "            deconv_1_1 = deconv_layer(deconv_1_2, 3, 3, 1, 'deconv1_1')\n",
    "        \n",
    "        return deconv_1_1\n",
    "        \n",
    "    def C1(self, input):\n",
    "        input = tf.image.resize_images(input, [227, 227])\n",
    "        with tf.variable_scope('C1', reuse=self.has_defined_C1):\n",
    "            conv1 = conv(input, 11, 96, 4, padding='VALID', name='conv1', trainable=False)\n",
    "            pool1 = max_pool(conv1, 3, 2, padding='VALID', name='pool1')\n",
    "            norm1 = lrn(pool1, 2, 2e-5, 0.75, name='norm1')\n",
    "\n",
    "            conv2 = conv(norm1, 5, 256, 1, groups=2, name='conv2', trainable=False)\n",
    "            pool2 = max_pool(conv2, 3, 2, padding='VALID', name='pool2')\n",
    "            norm2 = lrn(pool2, 2, 2e-5, 0.75, name='norm2')\n",
    "\n",
    "            conv3 = conv(norm2, 3, 384, 1, name='conv3', trainable=False)\n",
    "            conv4 = conv(conv3, 3, 384, 1, groups=2, name='conv4', trainable=False)\n",
    "            conv5 = conv(conv4, 3, 256, 1, groups=2, name='conv5', trainable=False)\n",
    "        self.has_defined_C1 = True\n",
    "        return conv5\n",
    "    \n",
    "    def init_weights(self, sess, alexnet_file, vgg_file):\n",
    "        weights_dict = np.load(alexnet_file, encoding='bytes').item()\n",
    "        with tf.variable_scope('C1', reuse=True):\n",
    "            for layer in ['conv1', 'conv2', 'conv3', 'conv4', 'conv5']:\n",
    "                with tf.variable_scope(layer):\n",
    "                    W_value, b_value = weights_dict[layer]\n",
    "                    W = tf.get_variable('W', trainable=False)\n",
    "                    b = tf.get_variable('b', trainable=False)\n",
    "                    sess.run(W.assign(W_value))\n",
    "                    sess.run(b.assign(b_value))\n",
    "        weights_dict = np.load(vgg_file, encoding='bytes').item()\n",
    "        weights_dict = { key.decode('ascii') : value for key, value in weights_dict.items() }\n",
    "        with tf.variable_scope('generator/f_img', reuse=True):\n",
    "            for layer in ['conv1_1', 'conv1_2',\n",
    "                          'conv2_1', 'conv2_2',\n",
    "                          'conv3_1', 'conv3_2', 'conv3_3',\n",
    "                          'conv4_1', 'conv4_2', 'conv4_3',\n",
    "                          'conv5_1', 'conv5_2', 'conv5_3',\n",
    "                          'fc6', 'fc7']:\n",
    "                with tf.variable_scope(layer):\n",
    "                    W_value, b_value = weights_dict[layer]\n",
    "                    W = tf.get_variable('W')\n",
    "                    b = tf.get_variable('b')\n",
    "                    sess.run(W.assign(W_value))\n",
    "                    sess.run(b.assign(b_value))\n",
    "    \n",
    "    def fit_batch(self,sess, p_t, p_t_n, x_t, x_t_n):\n",
    "        _, loss, summaries = sess.run((self.opt, self.loss, self.summaries), feed_dict={ self.p_t : p_t, self.p_t_n : p_t_n, self.x_t : x_t, self.x_t_n : x_t_n })\n",
    "        return loss, summaries\n",
    "\n",
    "class Discriminator(object):\n",
    "    def __init__(self):\n",
    "        self.train_variables = []\n",
    "        self.has_defined_layers = False\n",
    "    \n",
    "    def init_network(self, discriminator):\n",
    "        self.p_t = tf.placeholder(tf.float32, [None, 224, 224,L])\n",
    "        self.p_t_n = tf.placeholder(tf.float32, [None, 224, 224,L])\n",
    "        self.x_t = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        self.x_t_n = tf.placeholder(tf.float32, shape=[None, 224, 224, 3])\n",
    "        x_t_n_real = self.x_t_n\n",
    "        x_t_n_pred = generator.get_output_tensor(self.p_t_n, self.p_t, self.x_t)\n",
    "\n",
    "        real_prob = self.get_output_tensor(x_t_n_real, self.p_t_n)\n",
    "        fake_prob = self.get_output_tensor(x_t_n_pred, self.p_t_n)\n",
    "        real_mismatch_prob = self.get_output_tensor(self.x_t, self.p_t_n)\n",
    "        \n",
    "        real_loss = -tf.reduce_mean(tf.log(real_prob))\n",
    "        fake_loss = -tf.reduce_mean(tf.log(1 - fake_prob))\n",
    "        mismatch_loss = -tf.reduce_mean(tf.log(1 - real_mismatch_prob))\n",
    "        self.loss = real_loss + 0.5 * fake_loss + 0.5 * mismatch_loss\n",
    "        self.opt = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(self.loss, var_list=self.train_variables)\n",
    "        with tf.name_scope('discriminator'):\n",
    "            real_loss_summ = tf.summary.scalar('real_loss', real_loss)\n",
    "            fake_loss_summ = tf.summary.scalar('fake_loss', fake_loss)\n",
    "            mismatch_loss_summ = tf.summary.scalar('mismatch_loss', mismatch_loss)\n",
    "            loss_summ = tf.summary.scalar('loss', self.loss)\n",
    "            self.summaries = tf.summary.merge([real_loss_summ, fake_loss_summ, mismatch_loss_summ, loss_summ])\n",
    "            \n",
    "    def get_output_tensor(self, x, p):\n",
    "        with tf.variable_scope('discriminator', reuse=self.has_defined_layers):\n",
    "            with tf.variable_scope('f_img'):\n",
    "                vgg_x = vgg(x)\n",
    "            with tf.variable_scope('f_pose'):\n",
    "                vgg_p = vgg_simple(p)\n",
    "            concat = tf.concat([vgg_x, vgg_p], axis=1)\n",
    "            fc8 = fc(concat, 1024, name='fc8')\n",
    "            output = tf.nn.sigmoid(fc(fc8, 1, name='fc9', relu=False))\n",
    "        if not self.has_defined_layers:\n",
    "            self.train_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "        self.has_defined_layers = True\n",
    "        return output\n",
    "    \n",
    "    def init_weights(self, sess, alexnet_file, vgg_file):\n",
    "        weights_dict = np.load(vgg_file, encoding='bytes').item()\n",
    "        weights_dict = { key.decode('ascii') : value for key, value in weights_dict.items() }\n",
    "        with tf.variable_scope('discriminator/f_img', reuse=True):\n",
    "            for layer in ['conv1_1', 'conv1_2',\n",
    "                          'conv2_1', 'conv2_2',\n",
    "                          'conv3_1', 'conv3_2', 'conv3_3',\n",
    "                          'conv4_1', 'conv4_2', 'conv4_3',\n",
    "                          'conv5_1', 'conv5_2', 'conv5_3',\n",
    "                          'fc6', 'fc7']:\n",
    "                with tf.variable_scope(layer):\n",
    "                    W_value, b_value = weights_dict[layer]\n",
    "                    W = tf.get_variable('W')\n",
    "                    b = tf.get_variable('b')\n",
    "                    sess.run(W.assign(W_value))\n",
    "                    sess.run(b.assign(b_value))\n",
    "    \n",
    "    def fit_batch(self, sess, p_t, p_t_n, x_t, x_t_n):\n",
    "        _, loss, summaries = sess.run((self.opt, self.loss, self.summaries), feed_dict={ self.p_t : p_t, self.p_t_n : p_t_n, self.x_t : x_t, self.x_t_n : x_t_n })\n",
    "        return loss, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-06T07:36:57.685450",
     "start_time": "2017-05-06T00:36:57.536783-07:00"
    },
    "code_folding": [],
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def restore(generator, discriminator, checkpoint):\n",
    "    var_list = generator.train_variables + discriminator.train_variables\n",
    "    \n",
    "#     saver = tf.train.Saver(var_list={var.name.split(':')[0].replace(, 'alexnet') : var for var in var_list})\n",
    "    saver.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-06T07:36:57.792490",
     "start_time": "2017-05-06T00:36:57.689950-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-06T07:36:58.137Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "summary_writer = tf.summary.FileWriter('summaries/', graph=sess.graph)\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.init_network(discriminator)\n",
    "discriminator.init_network(generator)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "generator.init_weights(sess, 'models/alexnet.npy', 'models/vgg16.npy')\n",
    "discriminator.init_weights(sess, 'models/alexnet.npy', 'models/vgg16.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of squat videos. \n",
    "# Key = video number, Value = list of frames (numpy array images) of video\n",
    "videos = {}\n",
    "for video in os.listdir('squats/'):\n",
    "    videos[video] = []\n",
    "    for frame in os.listdir('squats/' + str(video) + '/'):\n",
    "        filename = 'squats/' + str(video) + '/' + str(frame)\n",
    "        videos[video].append(imread(filename))\n",
    "\n",
    "# Create dictionary of heatmat labels for squat videos. \n",
    "# For L = 13\n",
    "# Key = video number, Value = list of stack of joints (numpy array images (224x224x13))\n",
    "if L == 13:\n",
    "    labels = {}\n",
    "    for video in os.listdir('squats_labels_multiple/'):\n",
    "        if video in videos:\n",
    "            labels[video] = []\n",
    "            for frame in os.listdir('squats_labels_multiple/' + str(video) + '/'):\n",
    "                frame_folder = 'squats_labels_multiple/' + str(video) + '/' + str(frame) + '/'\n",
    "                temp_image_stack = np.zeros((224,224,13))\n",
    "                i = 0\n",
    "                for filename in os.listdir(frame_folder):\n",
    "                    temp_image_stack[:,:,i] = imread(frame_folder + filename)\n",
    "                    i = i + 1\n",
    "                labels[video].append(temp_image_stack)\n",
    "\n",
    "# For L = 1 \n",
    "# Key = video number, Value = list of heatmaps for each frame (numpy array images (224x224x1))            \n",
    "elif L == 1:\n",
    "    labels = {}\n",
    "    for video in os.listdir('squats_labels/'):\n",
    "        if video in videos:\n",
    "            labels[video] = []\n",
    "            for frame in os.listdir('squats_labels/' + str(video) + '/'):\n",
    "                filename = 'squats_labels/' + str(video) + '/' + str(frame)\n",
    "                labels[video].append(imread(filename).reshape((224,224,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_minibatch(batch_size):\n",
    "    frames1 = []\n",
    "    heatmaps1 = []\n",
    "    frames2 = []\n",
    "    heatmaps2 = []\n",
    "    for i in range(batch_size):\n",
    "        rand_video = videos.keys()[random.randint(0,len(videos.keys())-1)]\n",
    "        \n",
    "        rand_int = random.randint(0,len(videos[rand_video])-1)\n",
    "        frames1.append(videos[rand_video][rand_int])\n",
    "        heatmaps1.append(labels[rand_video][rand_int])\n",
    "\n",
    "        rand_int = random.randint(0,len(videos[rand_video])-1)\n",
    "        frames2.append(videos[rand_video][rand_int])\n",
    "        heatmaps2.append(labels[rand_video][rand_int])\n",
    "    return frames1, frames2, heatmaps1, heatmaps2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_samples = 1000\n",
    "batch_size = 2\n",
    "summary_freq_iter = 10\n",
    "display_step = 1\n",
    "\n",
    "mean_gen_losses = []\n",
    "mean_disc_losses = []\n",
    "for epoch in range(epochs):\n",
    "    total_iter = n_samples // batch_size\n",
    "    total_gen_loss = 0\n",
    "    total_disc_loss = 0\n",
    "    for i in range(total_iter):\n",
    "        f1,f2,h1,h2 = create_minibatch(batch_size)\n",
    "        gen_loss, gen_summaries = generator.fit_batch(sess,h1,h2,f1,f2)\n",
    "        disc_loss, disc_summaries = discriminator.fit_batch(sess,h1,h2,f1,f2)\n",
    "        total_gen_loss += gen_loss\n",
    "        total_disc_loss += disc_loss\n",
    "        if i % summary_freq_iter == 0:\n",
    "            step = epoch * n_samples + (i + 1) * batch_size\n",
    "            summary_writer.add_summary(gen_summaries, step)\n",
    "            summary_writer.add_summary(disc_summaries, step)\n",
    "    mean_gen_loss = total_gen_loss / total_iter\n",
    "    mean_disc_loss = total_disc_loss / total_iter\n",
    "    mean_gen_losses.append(mean_gen_loss)\n",
    "    mean_disc_losses.append(mean_disc_loss)\n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print('epoch %s: gen_loss=%.4f, disc_loss=%.4f' % (epoch + 1, mean_gen_loss, mean_disc_loss))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '/media/jeffzhang/WD HDD/model/multi-labels-test3-gradient-opt-100-100-005',global_step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
